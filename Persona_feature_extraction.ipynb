{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Persona feature extraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mazUbdIjLxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read personas file\n",
        "import pandas as pd\n",
        "data = pd.read_csv('Personas.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-wdsWLbp6SS",
        "colab_type": "code",
        "outputId": "096c067f-1bee-41f4-e14d-ced228ddf33b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Niche</th>\n",
              "      <th>Bundle</th>\n",
              "      <th>Location</th>\n",
              "      <th>Vertical</th>\n",
              "      <th>Role/Title</th>\n",
              "      <th>Gender (If Applicable)</th>\n",
              "      <th>Experience Level (Optional)</th>\n",
              "      <th>People/Friends in Niche That You Know</th>\n",
              "      <th>Describe Themselves As</th>\n",
              "      <th>Company Size Minimum (Employees)</th>\n",
              "      <th>Education Level (If Applicable)</th>\n",
              "      <th>Ticket Price</th>\n",
              "      <th>Revenue</th>\n",
              "      <th>Runway</th>\n",
              "      <th>Cash</th>\n",
              "      <th>Convresion rates</th>\n",
              "      <th>Current Activities/Responsibility</th>\n",
              "      <th>LinkedIn Company Interests</th>\n",
              "      <th>LinkedIn Influencers Followed</th>\n",
              "      <th>LinkedIn Groups They Belong To</th>\n",
              "      <th>Books They Read</th>\n",
              "      <th>Tools They Use</th>\n",
              "      <th>Blogs They Read</th>\n",
              "      <th>Forums/Subreddits They Browse</th>\n",
              "      <th>YouTube Channels They Watch</th>\n",
              "      <th>Facebook Likes</th>\n",
              "      <th>Slack Groups They Belong To</th>\n",
              "      <th>Podcasts They Listen To</th>\n",
              "      <th>Business Dreams and Desires</th>\n",
              "      <th>Personal Dreams and Desires</th>\n",
              "      <th>Failures</th>\n",
              "      <th>Fears (Willing to admit)</th>\n",
              "      <th>Suspicions</th>\n",
              "      <th>Enemies</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B2B SaaS Founders| CEO | Product Owners</td>\n",
              "      <td>B2B Online Entrepreneurs and Co-Founders with ...</td>\n",
              "      <td>UK</td>\n",
              "      <td>SaaS, Technology and Software Development: {Te...</td>\n",
              "      <td>CEO</td>\n",
              "      <td>Male</td>\n",
              "      <td>&gt; 5 Years</td>\n",
              "      <td>Jafer Lofti , Vanessa,Oliver</td>\n",
              "      <td>Use really cool tech, Little sales and marketi...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Post-secondary education in computer science, ...</td>\n",
              "      <td>&gt;= $3k</td>\n",
              "      <td>&gt;= $50k</td>\n",
              "      <td>&lt; 1 year</td>\n",
              "      <td>&lt; 500k</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{We need to understand what these people are d...</td>\n",
              "      <td>Muellners,A Lab Amsterdam,OpenRisk,Algoritmica...</td>\n",
              "      <td>Melinda Gates,Elon Musk ,Jeff Bezos ,Mark Zuck...</td>\n",
              "      <td>SaaS (Software as a Service) Networkers,14 Min...</td>\n",
              "      <td>The Lean Startup by Eric Ries,Zero to One by P...</td>\n",
              "      <td>The Lean Startup by Eric Ries,Zero to One by P...</td>\n",
              "      <td>Y Combinator,Andreessen Horowitz,Tom Tungsz,Bi...</td>\n",
              "      <td>Y Combinator Hacker News</td>\n",
              "      <td>Y Combinator</td>\n",
              "      <td>Jordan Belfort\\nSam Ovens\\nBloomberg\\nClearban...</td>\n",
              "      <td>TechMasters</td>\n",
              "      <td>The Tim Ferriss Show \\nHow I Built This \\nNath...</td>\n",
              "      <td>I am trying to build something durable, I am t...</td>\n",
              "      <td>Notes: What these people want at the end of th...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Running out of money before the inflection poi...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B2B SaaS Marketing Agency Owners (Product + Se...</td>\n",
              "      <td>B2B SaaS Marketing Agenceis Owners, Partners a...</td>\n",
              "      <td>NL</td>\n",
              "      <td>B2B SaaS Marketing Companies Owners</td>\n",
              "      <td>Owners</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&gt;2 Years</td>\n",
              "      <td>Bastiaan Beukeboom</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Content Writers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NL</td>\n",
              "      <td>B2B SaaS Content Marketing Experts</td>\n",
              "      <td>Content Experts</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&gt;2 Years</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>News Media Groups</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NL</td>\n",
              "      <td>New Media Groups</td>\n",
              "      <td>Owners</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&gt;2 Years</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Niche  ... Enemies\n",
              "0            B2B SaaS Founders| CEO | Product Owners  ...     NaN\n",
              "1  B2B SaaS Marketing Agency Owners (Product + Se...  ...     NaN\n",
              "2                                    Content Writers  ...     NaN\n",
              "3                                  News Media Groups  ...     NaN\n",
              "\n",
              "[4 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wvUuvcVqnIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dreams = data.iloc[:1,-6:-4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llMP00TfsbZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get busisness and personal dreams \n",
        "b_dreams = dreams.iloc[:,0].tolist()[0]\n",
        "p_dreams = dreams.iloc[:,1].tolist()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnpfegF7u3YU",
        "colab_type": "code",
        "outputId": "fe100526-ac93-480c-a2f3-0bdb2d525cf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "b_dreams,p_dreams"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('I am trying to build something durable, I am trying to retire my parenets, what they are willing to admit and what they are doing under hood. I am trying to be recognized, accumulate capital. We can hit the nerve under the hood. Self actualization comes after basic things solved. You cant be philanthropic. you have to help first yourself before helping others. We need to solve first order problems. ?? They want to build and sell business. They understand the investmet game well. \\nGet thank you notes from customers, create a few jobs, sponsor a few picnics, sell for a cool 5-10x rev, dabble with some charity, and feel at least 1/10th the way Warren feels when he dances off to work.  \\nBuild products that people find useful.\\nDo meaningful and interesting work. \\nFoster a great culture of talented people and a fun working environment.  \\nCreate jobs and contribute to the community. \\nEventually sell for 5-10x revenue or 15-30x EBITDA without bringing on a load of investors.\\n\\nEventually push forth a charitable agenda. \\nLess churn, less CAC\\nDo not give away equity\\nIf you are trying to raise your next round, stop and stick around\\n7 Step Process to Turn Your SaaS Business into Growth Machine If You Don’t Want to Give Up Equity\\n\\nAsk the prospect - “If you could fix one problem in your life such that by fixing it, everything else would be irrelevant, what would that be?”\\n“What is screaming at you? Something you can’t ignore. Something you think about the moment you wake up?”',\n",
              " 'Notes: What these people want at the end of the day. We are not just B2B, B2C...we are going into customer world. These are our heros. These are our guys, without them we are nothing. We are not passing judgement, but we are scientist. We acknowledge this person as much as humanly possible. They will tell you these things when you work with them. When they are under stress what they do say. We need to get 1m in less than a year as personal desire. \\nBuy the house of their dreams\\nBe mentioned in alumni publications \\nDrive a Porsche or Tesla\\nDonate money to causes\\nMake their spouse, parents, children, and close friends proud\\nSend kids to private school\\nProudly talk of their success at Thanksgiving with family and friends\\nBe a model citizen; successful, kind, intelligent, generous, and humble\\nHave the financial freedom to travel the world with their kids\\nBuy your wife a new car')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm7rOjk4vbtn",
        "colab_type": "code",
        "outputId": "06caace1-d5b1-4900-a2f0-2a62b93708b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "# install rake_nltk\n",
        "!pip install rake-nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from rake_nltk import Rake\n",
        "\n",
        "r = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n",
        "\n",
        "r.extract_keywords_from_text(b_dreams+\" \"+p_dreams)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rake-nltk\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/c4/b4ff57e541ac5624ad4b20b89c2bafd4e98f29fd83139f3a81858bdb3815/rake_nltk-1.0.4.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rake-nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->rake-nltk) (1.12.0)\n",
            "Building wheels for collected packages: rake-nltk\n",
            "  Building wheel for rake-nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rake-nltk: filename=rake_nltk-1.0.4-py2.py3-none-any.whl size=7819 sha256=2967546c25d8a65178ed609010a113e9cc8b5945b5b0ab3ad82dd7985525f771\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/92/fc/271b3709e71a96ffe934b27818946b795ac6b9b8ff8682483f\n",
            "Successfully built rake-nltk\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.4\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktLtBn0Nv_Oo",
        "colab_type": "code",
        "outputId": "a8e8f81e-657e-4a2b-cbea-8a8110c197d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "r.get_ranked_phrases_with_scores() # To get keyword phrases ranked highest to lowest."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(25.0, 'stick around 7 step process'),\n",
              " (21.5, 'close friends proud send kids'),\n",
              " (16.0, 'private school proudly talk'),\n",
              " (16.0, 'could fix one problem'),\n",
              " (15.0, 'solve first order problems'),\n",
              " (14.5, '30x ebitda without bringing'),\n",
              " (9.0, 'way warren feels'),\n",
              " (9.0, 'tesla donate money'),\n",
              " (9.0, 'self actualization comes'),\n",
              " (9.0, 'investmet game well'),\n",
              " (9.0, 'fun working environment'),\n",
              " (9.0, 'alumni publications drive'),\n",
              " (8.5, 'eventually push forth'),\n",
              " (8.333333333333334, 'people find useful'),\n",
              " (8.0, 'everything else would'),\n",
              " (8.0, 'basic things solved'),\n",
              " (7.5, 'give away equity'),\n",
              " (7.0, 'build something durable'),\n",
              " (5.0, 'kids buy'),\n",
              " (5.0, 'help first'),\n",
              " (4.5, 'equity ask'),\n",
              " (4.333333333333334, 'talented people'),\n",
              " (4.166666666666667, 'eventually sell'),\n",
              " (4.0, 'saas business'),\n",
              " (4.0, 'personal desire'),\n",
              " (4.0, 'passing judgement'),\n",
              " (4.0, 'next round'),\n",
              " (4.0, 'new car'),\n",
              " (4.0, 'model citizen'),\n",
              " (4.0, 'least 1'),\n",
              " (4.0, 'humanly possible'),\n",
              " (4.0, 'helping others'),\n",
              " (4.0, 'growth machine'),\n",
              " (4.0, 'great culture'),\n",
              " (4.0, 'get thank'),\n",
              " (4.0, 'get 1m'),\n",
              " (4.0, 'financial freedom'),\n",
              " (4.0, 'charitable agenda'),\n",
              " (4.0, 'causes make'),\n",
              " (4.0, 'build products'),\n",
              " (4.0, 'b2c ...'),\n",
              " (4.0, 'accumulate capital'),\n",
              " (4.0, '10x revenue'),\n",
              " (4.0, '10x rev'),\n",
              " (3.8333333333333335, 'people want'),\n",
              " (3.666666666666667, 'sell business'),\n",
              " (3.666666666666667, 'less churn'),\n",
              " (3.666666666666667, 'less cac'),\n",
              " (3.5, 'interesting work'),\n",
              " (3.5, 'customer world'),\n",
              " (3.5, 'cool 5'),\n",
              " (3.5, '?” “'),\n",
              " (3.5, '?” notes'),\n",
              " (3.0, 'friends'),\n",
              " (3.0, 'create jobs'),\n",
              " (2.5, 'without'),\n",
              " (2.0, 'would'),\n",
              " (2.0, 'things'),\n",
              " (2.0, 'something'),\n",
              " (2.0, 'give'),\n",
              " (2.0, 'build'),\n",
              " (1.6666666666666667, 'sell'),\n",
              " (1.6666666666666667, 'less'),\n",
              " (1.5, '“'),\n",
              " (1.5, 'world'),\n",
              " (1.5, 'work'),\n",
              " (1.5, 'want'),\n",
              " (1.5, 'notes'),\n",
              " (1.5, 'jobs'),\n",
              " (1.5, 'create'),\n",
              " (1.5, 'buy'),\n",
              " (1.5, '5'),\n",
              " (1.0, '’'),\n",
              " (1.0, 'year'),\n",
              " (1.0, 'willing'),\n",
              " (1.0, 'wife'),\n",
              " (1.0, 'wake'),\n",
              " (1.0, 'understand'),\n",
              " (1.0, 'turn'),\n",
              " (1.0, 'trying'),\n",
              " (1.0, 'travel'),\n",
              " (1.0, 'think'),\n",
              " (1.0, 'thanksgiving'),\n",
              " (1.0, 'tell'),\n",
              " (1.0, 'successful'),\n",
              " (1.0, 'success'),\n",
              " (1.0, 'stress'),\n",
              " (1.0, 'stop'),\n",
              " (1.0, 'spouse'),\n",
              " (1.0, 'sponsor'),\n",
              " (1.0, 'screaming'),\n",
              " (1.0, 'scientist'),\n",
              " (1.0, 'say'),\n",
              " (1.0, 'retire'),\n",
              " (1.0, 'recognized'),\n",
              " (1.0, 'raise'),\n",
              " (1.0, 'prospect'),\n",
              " (1.0, 'porsche'),\n",
              " (1.0, 'picnics'),\n",
              " (1.0, 'philanthropic'),\n",
              " (1.0, 'person'),\n",
              " (1.0, 'parents'),\n",
              " (1.0, 'parenets'),\n",
              " (1.0, 'nothing'),\n",
              " (1.0, 'nerve'),\n",
              " (1.0, 'need'),\n",
              " (1.0, 'much'),\n",
              " (1.0, 'moment'),\n",
              " (1.0, 'mentioned'),\n",
              " (1.0, 'meaningful'),\n",
              " (1.0, 'load'),\n",
              " (1.0, 'life'),\n",
              " (1.0, 'kind'),\n",
              " (1.0, 'irrelevant'),\n",
              " (1.0, 'investors'),\n",
              " (1.0, 'intelligent'),\n",
              " (1.0, 'ignore'),\n",
              " (1.0, 'humble'),\n",
              " (1.0, 'house'),\n",
              " (1.0, 'hood'),\n",
              " (1.0, 'hit'),\n",
              " (1.0, 'heros'),\n",
              " (1.0, 'guys'),\n",
              " (1.0, 'going'),\n",
              " (1.0, 'generous'),\n",
              " (1.0, 'foster'),\n",
              " (1.0, 'fixing'),\n",
              " (1.0, 'feel'),\n",
              " (1.0, 'family'),\n",
              " (1.0, 'end'),\n",
              " (1.0, 'dreams'),\n",
              " (1.0, 'day'),\n",
              " (1.0, 'dances'),\n",
              " (1.0, 'dabble'),\n",
              " (1.0, 'customers'),\n",
              " (1.0, 'contribute'),\n",
              " (1.0, 'community'),\n",
              " (1.0, 'children'),\n",
              " (1.0, 'charity'),\n",
              " (1.0, 'cant'),\n",
              " (1.0, 'b2b'),\n",
              " (1.0, 'admit'),\n",
              " (1.0, 'acknowledge'),\n",
              " (1.0, '??'),\n",
              " (1.0, '15'),\n",
              " (1.0, '10th')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2XIIFkcXmJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import en_core_web_sm\n",
        "\n",
        "# use spacy small model\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "# dependency markers for subjects\n",
        "SUBJECTS = {\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"}\n",
        "# dependency markers for objects\n",
        "OBJECTS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
        "# POS tags that will break adjoining items\n",
        "BREAKER_POS = {\"CCONJ\", \"VERB\"}\n",
        "# words that are negations\n",
        "NEGATIONS = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
        "\n",
        "\n",
        "# does dependency set contain any coordinating conjunctions?\n",
        "def contains_conj(depSet):\n",
        "    return \"and\" in depSet or \"or\" in depSet or \"nor\" in depSet or \\\n",
        "           \"but\" in depSet or \"yet\" in depSet or \"so\" in depSet or \"for\" in depSet\n",
        "\n",
        "\n",
        "# get subs joined by conjunctions\n",
        "def _get_subs_from_conjunctions(subs):\n",
        "    more_subs = []\n",
        "    for sub in subs:\n",
        "        # rights is a generator\n",
        "        rights = list(sub.rights)\n",
        "        rightDeps = {tok.lower_ for tok in rights}\n",
        "        if contains_conj(rightDeps):\n",
        "            more_subs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
        "            if len(more_subs) > 0:\n",
        "                more_subs.extend(_get_subs_from_conjunctions(more_subs))\n",
        "    return more_subs\n",
        "\n",
        "\n",
        "# get objects joined by conjunctions\n",
        "def _get_objs_from_conjunctions(objs):\n",
        "    more_objs = []\n",
        "    for obj in objs:\n",
        "        # rights is a generator\n",
        "        rights = list(obj.rights)\n",
        "        rightDeps = {tok.lower_ for tok in rights}\n",
        "        if contains_conj(rightDeps):\n",
        "            more_objs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
        "            if len(more_objs) > 0:\n",
        "                more_objs.extend(_get_objs_from_conjunctions(more_objs))\n",
        "    return more_objs\n",
        "\n",
        "\n",
        "# find sub dependencies\n",
        "def _find_subs(tok):\n",
        "    head = tok.head\n",
        "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
        "        head = head.head\n",
        "    if head.pos_ == \"VERB\":\n",
        "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
        "        if len(subs) > 0:\n",
        "            verb_negated = _is_negated(head)\n",
        "            subs.extend(_get_subs_from_conjunctions(subs))\n",
        "            return subs, verb_negated\n",
        "        elif head.head != head:\n",
        "            return _find_subs(head)\n",
        "    elif head.pos_ == \"NOUN\":\n",
        "        return [head], _is_negated(tok)\n",
        "    return [], False\n",
        "\n",
        "\n",
        "# is the tok set's left or right negated?\n",
        "def _is_negated(tok):\n",
        "    parts = list(tok.lefts) + list(tok.rights)\n",
        "    for dep in parts:\n",
        "        if dep.lower_ in NEGATIONS:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# get all the verbs on tokens with negation marker\n",
        "def _find_svs(tokens):\n",
        "    svs = []\n",
        "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n",
        "    for v in verbs:\n",
        "        subs, verbNegated = _get_all_subs(v)\n",
        "        if len(subs) > 0:\n",
        "            for sub in subs:\n",
        "                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n",
        "    return svs\n",
        "\n",
        "\n",
        "# get grammatical objects for a given set of dependencies (including passive sentences)\n",
        "def _get_objs_from_prepositions(deps, is_pas):\n",
        "    objs = []\n",
        "    for dep in deps:\n",
        "        if dep.pos_ == \"ADP\" and (dep.dep_ == \"prep\" or (is_pas and dep.dep_ == \"agent\")):\n",
        "            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or\n",
        "                         (tok.pos_ == \"PRON\" and tok.lower_ == \"me\") or\n",
        "                         (is_pas and tok.dep_ == 'pobj')])\n",
        "    return objs\n",
        "\n",
        "\n",
        "# get objects from the dependencies using the attribute dependency\n",
        "def _get_objs_from_attrs(deps, is_pas):\n",
        "    for dep in deps:\n",
        "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
        "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
        "            if len(verbs) > 0:\n",
        "                for v in verbs:\n",
        "                    rights = list(v.rights)\n",
        "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
        "                    objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
        "                    if len(objs) > 0:\n",
        "                        return v, objs\n",
        "    return None, None\n",
        "\n",
        "\n",
        "# xcomp; open complement - verb has no suject\n",
        "def _get_obj_from_xcomp(deps, is_pas):\n",
        "    for dep in deps:\n",
        "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
        "            v = dep\n",
        "            rights = list(v.rights)\n",
        "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
        "            objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
        "            if len(objs) > 0:\n",
        "                return v, objs\n",
        "    return None, None\n",
        "\n",
        "\n",
        "# get all functional subjects adjacent to the verb passed in\n",
        "def _get_all_subs(v):\n",
        "    verb_negated = _is_negated(v)\n",
        "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
        "    if len(subs) > 0:\n",
        "        subs.extend(_get_subs_from_conjunctions(subs))\n",
        "    else:\n",
        "        foundSubs, verb_negated = _find_subs(v)\n",
        "        subs.extend(foundSubs)\n",
        "    return subs, verb_negated\n",
        "\n",
        "\n",
        "# is the token a verb?  (excluding auxiliary verbs)\n",
        "def _is_non_aux_verb(tok):\n",
        "    return tok.pos_ == \"VERB\" and (tok.dep_ != \"aux\" and tok.dep_ != \"auxpass\")\n",
        "\n",
        "\n",
        "# return the verb to the right of this verb in a CCONJ relationship if applicable\n",
        "# returns a tuple, first part True|False and second part the modified verb if True\n",
        "def _right_of_verb_is_conj_verb(v):\n",
        "    # rights is a generator\n",
        "    rights = list(v.rights)\n",
        "\n",
        "    # VERB CCONJ VERB (e.g. he beat and hurt me)\n",
        "    if len(rights) > 1 and rights[0].pos_ == 'CCONJ':\n",
        "        for tok in rights[1:]:\n",
        "            if _is_non_aux_verb(tok):\n",
        "                return True, tok\n",
        "\n",
        "    return False, v\n",
        "\n",
        "\n",
        "# get all objects for an active/passive sentence\n",
        "def _get_all_objs(v, is_pas):\n",
        "    # rights is a generator\n",
        "    rights = list(v.rights)\n",
        "\n",
        "    objs = [tok for tok in rights if tok.dep_ in OBJECTS or (is_pas and tok.dep_ == 'pobj')]\n",
        "    objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
        "\n",
        "    #potentialNewVerb, potentialNewObjs = _get_objs_from_attrs(rights)\n",
        "    #if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
        "    #    objs.extend(potentialNewObjs)\n",
        "    #    v = potentialNewVerb\n",
        "\n",
        "    potential_new_verb, potential_new_objs = _get_obj_from_xcomp(rights, is_pas)\n",
        "    if potential_new_verb is not None and potential_new_objs is not None and len(potential_new_objs) > 0:\n",
        "        objs.extend(potential_new_objs)\n",
        "        v = potential_new_verb\n",
        "    if len(objs) > 0:\n",
        "        objs.extend(_get_objs_from_conjunctions(objs))\n",
        "    return v, objs\n",
        "\n",
        "\n",
        "# return true if the sentence is passive - at he moment a sentence is assumed passive if it has an auxpass verb\n",
        "def _is_passive(tokens):\n",
        "    for tok in tokens:\n",
        "        if tok.dep_ == \"auxpass\":\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# resolve a 'that' where/if appropriate\n",
        "def _get_that_resolution(toks):\n",
        "    for tok in toks:\n",
        "        if 'that' in [t.orth_ for t in tok.lefts]:\n",
        "            return tok.head\n",
        "    return toks\n",
        "\n",
        "\n",
        "# simple stemmer using lemmas\n",
        "def _get_lemma(word: str):\n",
        "    tokens = nlp(word)\n",
        "    if len(tokens) == 1:\n",
        "        return tokens[0].lemma_\n",
        "    return word\n",
        "\n",
        "\n",
        "# print information for displaying all kinds of things of the parse tree\n",
        "def printDeps(toks):\n",
        "    for tok in toks:\n",
        "        print(tok.orth_, tok.dep_, tok.pos_, tok.head.orth_, [t.orth_ for t in tok.lefts], [t.orth_ for t in tok.rights])\n",
        "\n",
        "\n",
        "# expand an obj / subj np using its chunk\n",
        "def expand(item, tokens, visited):\n",
        "    if item.lower_ == 'that':\n",
        "        item = _get_that_resolution(tokens)\n",
        "\n",
        "    parts = []\n",
        "\n",
        "    if hasattr(item, 'lefts'):\n",
        "        for part in item.lefts:\n",
        "            if part.pos_ in BREAKER_POS:\n",
        "                break\n",
        "            if not part.lower_ in NEGATIONS:\n",
        "                parts.append(part)\n",
        "\n",
        "    parts.append(item)\n",
        "\n",
        "    if hasattr(item, 'rights'):\n",
        "        for part in item.rights:\n",
        "            if part.pos_ in BREAKER_POS:\n",
        "                break\n",
        "            if not part.lower_ in NEGATIONS:\n",
        "                parts.append(part)\n",
        "\n",
        "    if hasattr(parts[-1], 'rights'):\n",
        "        for item2 in parts[-1].rights:\n",
        "            if item2.pos_ == \"DET\" or item2.pos_ == \"NOUN\":\n",
        "                if item2.i not in visited:\n",
        "                    visited.add(item2.i)\n",
        "                    parts.extend(expand(item2, tokens, visited))\n",
        "            break\n",
        "\n",
        "    return parts\n",
        "\n",
        "\n",
        "# convert a list of tokens to a string\n",
        "def to_str(tokens):\n",
        "    return ' '.join([item.text for item in tokens])\n",
        "\n",
        "\n",
        "# find verbs and their subjects / objects to create SVOs, detect passive/active sentences\n",
        "def findSVOs(tokens):\n",
        "    svos = []\n",
        "    is_pas = _is_passive(tokens)\n",
        "    verbs = [tok for tok in tokens if _is_non_aux_verb(tok)]\n",
        "    visited = set()  # recursion detection\n",
        "    for v in verbs:\n",
        "        subs, verbNegated = _get_all_subs(v)\n",
        "        # hopefully there are subs, if not, don't examine this verb any longer\n",
        "        if len(subs) > 0:\n",
        "            isConjVerb, conjV = _right_of_verb_is_conj_verb(v)\n",
        "            if isConjVerb:\n",
        "                v2, objs = _get_all_objs(conjV, is_pas)\n",
        "                for sub in subs:\n",
        "                    for obj in objs:\n",
        "                        objNegated = _is_negated(obj)\n",
        "                        if is_pas:  # reverse object / subject for passive\n",
        "                            svos.append((to_str(expand(obj, tokens, visited)),\n",
        "                                         \"!\" + v.lemma_ if verbNegated or objNegated else v.lemma_, to_str(expand(sub, tokens, visited))))\n",
        "                            svos.append((to_str(expand(obj, tokens, visited)),\n",
        "                                         \"!\" + v2.lemma_ if verbNegated or objNegated else v2.lemma_, to_str(expand(sub, tokens, visited))))\n",
        "                        else:\n",
        "                            svos.append((to_str(expand(sub, tokens, visited)),\n",
        "                                         \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, to_str(expand(obj, tokens, visited))))\n",
        "                            svos.append((to_str(expand(sub, tokens, visited)),\n",
        "                                         \"!\" + v2.lower_ if verbNegated or objNegated else v2.lower_, to_str(expand(obj, tokens, visited))))\n",
        "            else:\n",
        "                v, objs = _get_all_objs(v, is_pas)\n",
        "                for sub in subs:\n",
        "                    for obj in objs:\n",
        "                        objNegated = _is_negated(obj)\n",
        "                        if is_pas:  # reverse object / subject for passive\n",
        "                            svos.append((to_str(expand(obj, tokens, visited)),\n",
        "                                         \"!\" + v.lemma_ if verbNegated or objNegated else v.lemma_, to_str(expand(sub, tokens, visited))))\n",
        "                        else:\n",
        "                            svos.append((to_str(expand(sub, tokens, visited)),\n",
        "                                         \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, to_str(expand(obj, tokens, visited))))\n",
        "    return svos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyWJt1Qmx6Cv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keywords = r.get_ranked_phrases()\n",
        "\n",
        "# apply sentence tokenization\n",
        "sents = []\n",
        "sents = nltk.sent_tokenize(b_dreams+\". \"+p_dreams)\n",
        "\n",
        "# remove punctuation\n",
        "import string \n",
        "# remove punctuation dictionary\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "filteredSents = []\n",
        "for sent in sents:\n",
        "  filteredSents.append(sent.translate(remove_punct_dict))\n",
        "sents = filteredSents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHkCK-aUikh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "e301463e-25be-46ec-bd0c-286a3e251187"
      },
      "source": [
        "sents"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am trying to build something durable I am trying to retire my parenets what they are willing to admit and what they are doing under hood',\n",
              " 'I am trying to be recognized accumulate capital',\n",
              " 'We can hit the nerve under the hood',\n",
              " 'Self actualization comes after basic things solved',\n",
              " 'You cant be philanthropic',\n",
              " 'you have to help first yourself before helping others',\n",
              " 'We need to solve first order problems',\n",
              " '',\n",
              " 'They want to build and sell business',\n",
              " 'They understand the investmet game well',\n",
              " 'Get thank you notes from customers create a few jobs sponsor a few picnics sell for a cool 510x rev dabble with some charity and feel at least 110th the way Warren feels when he dances off to work',\n",
              " 'Build products that people find useful',\n",
              " 'Do meaningful and interesting work',\n",
              " 'Foster a great culture of talented people and a fun working environment',\n",
              " 'Create jobs and contribute to the community',\n",
              " 'Eventually sell for 510x revenue or 1530x EBITDA without bringing on a load of investors',\n",
              " 'Eventually push forth a charitable agenda',\n",
              " 'Less churn less CAC\\nDo not give away equity\\nIf you are trying to raise your next round stop and stick around\\n7 Step Process to Turn Your SaaS Business into Growth Machine If You Don’t Want to Give Up Equity\\n\\nAsk the prospect  “If you could fix one problem in your life such that by fixing it everything else would be irrelevant what would that be”\\n“What is screaming at you',\n",
              " 'Something you can’t ignore',\n",
              " 'Something you think about the moment you wake up”',\n",
              " 'Notes What these people want at the end of the day',\n",
              " 'We are not just B2B B2Cwe are going into customer world',\n",
              " 'These are our heros',\n",
              " 'These are our guys without them we are nothing',\n",
              " 'We are not passing judgement but we are scientist',\n",
              " 'We acknowledge this person as much as humanly possible',\n",
              " 'They will tell you these things when you work with them',\n",
              " 'When they are under stress what they do say',\n",
              " 'We need to get 1m in less than a year as personal desire',\n",
              " 'Buy the house of their dreams\\nBe mentioned in alumni publications \\nDrive a Porsche or Tesla\\nDonate money to causes\\nMake their spouse parents children and close friends proud\\nSend kids to private school\\nProudly talk of their success at Thanksgiving with family and friends\\nBe a model citizen successful kind intelligent generous and humble\\nHave the financial freedom to travel the world with their kids\\nBuy your wife a new car']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh7m104xigga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract subject verb object from keywords\n",
        "svo = []\n",
        "for sent in sents:\n",
        "  tokens = nlp(sent)\n",
        "  svos = findSVOs(tokens)\n",
        "  svo.append(svos)\n",
        "filtered_keywords = []\n",
        "for keyword in keywords:  \n",
        "  if len(keyword.split()) > 1:\n",
        "    filtered_keywords.append(keyword)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lLSgK3cdSei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "61792cf9-fa9a-439e-d498-0c25a5016314"
      },
      "source": [
        "svo"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('I', 'build', 'something durable'), ('I', 'retire', 'my parenets')],\n",
              " [('accumulate capital', 'recognize', 'I')],\n",
              " [('We', 'hit', 'the nerve')],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [('We', 'solve', 'order problems')],\n",
              " [],\n",
              " [],\n",
              " [('They', 'understand', 'the investmet game')],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [('CAC \\n', '!give', 'equity \\n'),\n",
              "  ('you', 'raise', 'your next round stop'),\n",
              "  ('You', 'give', 'Equity \\n\\n'),\n",
              "  ('you', 'fix', 'one problem')],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [('We', '!passing', 'judgement')],\n",
              " [('We', 'acknowledge', 'this person')],\n",
              " [('They', 'tell', 'you'), ('They', 'tell', 'these things')],\n",
              " [],\n",
              " [],\n",
              " [('alumni publications \\n', 'mention', 'the house of their dreams \\n'),\n",
              "  ('the world', 'travel', 'the financial freedom'),\n",
              "  ('their kids \\n', 'travel', 'the financial freedom')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LroWsRljZkwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filter svos\n",
        "sentsOfSVO = []\n",
        "for listOfSentences in svo:\n",
        "  for sentence in listOfSentences:    \n",
        "    subject,verb,obj = sentence\n",
        "    sentsOfSVO.append(verb+\" \"+obj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UBiFTq9abnB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "1c89920b-9c9f-4933-b14d-d882166a99c9"
      },
      "source": [
        "sentsOfSVO"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['build something durable',\n",
              " 'retire my parenets',\n",
              " 'recognize I',\n",
              " 'hit the nerve',\n",
              " 'solve order problems',\n",
              " 'understand the investmet game',\n",
              " '!give equity \\n',\n",
              " 'raise your next round stop',\n",
              " 'give Equity \\n\\n',\n",
              " 'fix one problem',\n",
              " '!passing judgement',\n",
              " 'acknowledge this person',\n",
              " 'tell you',\n",
              " 'tell these things',\n",
              " 'mention the house of their dreams \\n',\n",
              " 'travel the financial freedom',\n",
              " 'travel the financial freedom']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPjgJlQmyXlf",
        "colab_type": "code",
        "outputId": "a7e824c1-64a4-4843-fe42-a1933cdad0fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# can textrank improve the extracted dreams ?\n",
        "\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from pprint import pprint\n",
        "from nltk.cluster.util import cosine_distance\n",
        "\n",
        "MULTIPLE_WHITESPACE_PATTERN = re.compile(r\"\\s+\", re.UNICODE)\n",
        "nltk.download('punkt')\n",
        "\n",
        "def normalize_whitespace(text):\n",
        "    \"\"\"\n",
        "    Translates multiple whitespace into single space character.\n",
        "    If there is at least one new line character chunk is replaced\n",
        "    by single LF (Unix new line) character.\n",
        "    \"\"\"\n",
        "    return MULTIPLE_WHITESPACE_PATTERN.sub(_replace_whitespace, text)\n",
        "\n",
        "\n",
        "def _replace_whitespace(match):\n",
        "    text = match.group()\n",
        "\n",
        "    if \"\\n\" in text or \"\\r\" in text:\n",
        "        return \"\\n\"\n",
        "    else:\n",
        "        return \" \"\n",
        "\n",
        "\n",
        "def is_blank(string):\n",
        "    \"\"\"\n",
        "    Returns `True` if string contains only white-space characters\n",
        "    or is empty. Otherwise `False` is returned.\n",
        "    \"\"\"\n",
        "    return not string or string.isspace()\n",
        "\n",
        "\n",
        "def get_symmetric_matrix(matrix):\n",
        "    \"\"\"\n",
        "    Get Symmetric matrix\n",
        "    :param matrix:\n",
        "    :return: matrix\n",
        "    \"\"\"\n",
        "    return matrix + matrix.T - np.diag(matrix.diagonal())\n",
        "\n",
        "\n",
        "def core_cosine_similarity(vector1, vector2):\n",
        "    \"\"\"\n",
        "    measure cosine similarity between two vectors\n",
        "    :param vector1:\n",
        "    :param vector2:\n",
        "    :return: 0 < cosine similarity value < 1\n",
        "    \"\"\"\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "\n",
        "'''\n",
        "Note: This is not a summarization algorithm. This Algorithm pics top sentences irrespective of the order they appeared.\n",
        "'''\n",
        "\n",
        "\n",
        "class TextRank4Sentences():\n",
        "    def __init__(self):\n",
        "        self.damping = 0.85  # damping coefficient, usually is .85\n",
        "        self.min_diff = 1e-5  # convergence threshold\n",
        "        self.steps = 100  # iteration steps\n",
        "        self.text_str = None\n",
        "        self.sentences = None\n",
        "        self.pr_vector = None\n",
        "\n",
        "    def _sentence_similarity(self, sent1, sent2, stopwords=None):\n",
        "        if stopwords is None:\n",
        "            stopwords = []\n",
        "\n",
        "        sent1 = [w.lower() for w in sent1]\n",
        "        sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "        all_words = list(set(sent1 + sent2))\n",
        "\n",
        "        vector1 = [0] * len(all_words)\n",
        "        vector2 = [0] * len(all_words)\n",
        "\n",
        "        # build the vector for the first sentence\n",
        "        for w in sent1:\n",
        "            if w in stopwords:\n",
        "                continue\n",
        "            vector1[all_words.index(w)] += 1\n",
        "\n",
        "        # build the vector for the second sentence\n",
        "        for w in sent2:\n",
        "            if w in stopwords:\n",
        "                continue\n",
        "            vector2[all_words.index(w)] += 1\n",
        "\n",
        "        return core_cosine_similarity(vector1, vector2)\n",
        "\n",
        "    def _build_similarity_matrix(self, sentences, stopwords=None):\n",
        "        # create an empty similarity matrix\n",
        "        sm = np.zeros([len(sentences), len(sentences)])\n",
        "\n",
        "        for idx1 in range(len(sentences)):\n",
        "            for idx2 in range(len(sentences)):\n",
        "                if idx1 == idx2:\n",
        "                    continue\n",
        "\n",
        "                sm[idx1][idx2] = self._sentence_similarity(sentences[idx1], sentences[idx2], stopwords=stopwords)\n",
        "\n",
        "        # Get Symmeric matrix\n",
        "        sm = get_symmetric_matrix(sm)\n",
        "\n",
        "        # Normalize matrix by column\n",
        "        norm = np.sum(sm, axis=0)\n",
        "        sm_norm = np.divide(sm, norm, where=norm != 0)  # this is to ignore the 0 element in norm\n",
        "\n",
        "        return sm_norm\n",
        "\n",
        "    def _run_page_rank(self, similarity_matrix):\n",
        "\n",
        "        pr_vector = np.array([1] * len(similarity_matrix))\n",
        "\n",
        "        # Iteration\n",
        "        previous_pr = 0\n",
        "        for epoch in range(self.steps):\n",
        "            pr_vector = (1 - self.damping) + self.damping * np.matmul(similarity_matrix, pr_vector)\n",
        "            if abs(previous_pr - sum(pr_vector)) < self.min_diff:\n",
        "                break\n",
        "            else:\n",
        "                previous_pr = sum(pr_vector)\n",
        "\n",
        "        return pr_vector\n",
        "\n",
        "    def _get_sentence(self, index):\n",
        "\n",
        "        try:\n",
        "            return self.sentences[index]\n",
        "        except IndexError:\n",
        "            return \"\"\n",
        "\n",
        "    def get_top_sentences(self, number=0):\n",
        "\n",
        "        sorted_sent = []\n",
        "        if self.pr_vector is not None:\n",
        "\n",
        "            sorted_pr = np.argsort(self.pr_vector)\n",
        "            sorted_pr = list(sorted_pr)\n",
        "            sorted_pr.reverse()\n",
        "        \n",
        "            if number == 0 :\n",
        "                number = len(sorted_pr)\n",
        "\n",
        "            for index in range(number):\n",
        "                # print (str(sorted_pr[index]) + \" : \" + str(self.pr_vector[sorted_pr[index]]))\n",
        "                sent = self.sentences[sorted_pr[index]]\n",
        "                sent = normalize_whitespace(sent)\n",
        "                sorted_sent.append([sent,self.pr_vector[sorted_pr[index]]])\n",
        "            \n",
        "\n",
        "        return sorted_sent\n",
        "\n",
        "    def analyze(self, sentences, stop_words=None):\n",
        "        self.sentences = sentences\n",
        "\n",
        "        tokenized_sentences = [word_tokenize(sent) for sent in self.sentences]\n",
        "\n",
        "        similarity_matrix = self._build_similarity_matrix(tokenized_sentences, stop_words)\n",
        "\n",
        "        self.pr_vector = self._run_page_rank(similarity_matrix)\n",
        "        # print(self.pr_vector)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUAzdycmzLaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combine RAKE keywords with SVOs\n",
        "keywords.extend(sentsOfSVO)\n",
        "# remove duplicates from keywords\n",
        "keywords = list(dict.fromkeys(keywords))\n",
        "# apply text rank\n",
        "sentenceRanker = TextRank4Sentences()\n",
        "sentenceRanker.analyze(keywords)\n",
        "top_keywords = sentenceRanker.get_top_sentences()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzivgoqZfu28",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "68d03356-d7d0-4dca-ba29-327c9d6ea5d6"
      },
      "source": [
        "top_keywords"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['hit the nerve', 1.6970595408339264],\n",
              " ['travel the financial freedom', 1.6434831382341972],\n",
              " ['understand the investmet game', 1.6075978040249286],\n",
              " ['mention the house of their dreams\\n', 1.4444156452638794],\n",
              " ['tell these things', 1.389402560525686],\n",
              " ['raise your next round stop', 1.3718737942273989],\n",
              " ['sell', 1.367195026508615],\n",
              " ['raise your next round', 1.3533230859377086],\n",
              " ['!give equity\\n', 1.3454783712681637],\n",
              " ['?” notes', 1.3394415618653874],\n",
              " ['?” “', 1.3394415618653874],\n",
              " ['people want', 1.3033697763297756],\n",
              " ['sell business', 1.301052613582438],\n",
              " ['solve order problems', 1.2666354758946907],\n",
              " ['eventually sell', 1.251883850658229],\n",
              " ['give Equity\\n', 1.2315863756768777],\n",
              " ['kids buy', 1.221296048809605],\n",
              " ['build something durable', 1.1888804377556572],\n",
              " ['build', 1.134275968880904],\n",
              " ['solve first order problems', 1.1275],\n",
              " ['create jobs', 1.1275],\n",
              " ['acknowledge this person', 1.1275],\n",
              " ['retire my parenets', 1.1275],\n",
              " ['give away equity', 1.0988184018738951],\n",
              " ['??', 1.0855584381346126],\n",
              " ['less', 1.0732220356676665],\n",
              " ['close friends proud send kids', 1.0558744456325178],\n",
              " ['next round', 1.0198960275346058],\n",
              " ['without', 1.0],\n",
              " ['10x revenue', 1.0],\n",
              " ['10x rev', 1.0],\n",
              " ['5', 1.0],\n",
              " ['would', 1.0],\n",
              " ['get thank', 1.0],\n",
              " ['everything else would', 1.0],\n",
              " ['work', 1.0],\n",
              " ['fix one problem', 1.0],\n",
              " ['recognize I', 1.0],\n",
              " ['try I', 1.0],\n",
              " ['could fix one problem', 1.0],\n",
              " ['world', 1.0],\n",
              " ['30x ebitda without bringing', 1.0],\n",
              " ['cool 5', 1.0],\n",
              " ['customer world', 1.0],\n",
              " ['interesting work', 1.0],\n",
              " ['get 1m', 1.0],\n",
              " ['build products', 0.9952023210659772],\n",
              " ['tell', 0.9903203284020639],\n",
              " ['talented people', 0.9654577492232516],\n",
              " ['less churn', 0.9633889821661666],\n",
              " ['less cac', 0.9633889821661666],\n",
              " ['things', 0.9490885788792535],\n",
              " ['buy', 0.9441255543674822],\n",
              " ['retire', 0.93625],\n",
              " ['person', 0.93625],\n",
              " ['parenets', 0.93625],\n",
              " ['jobs', 0.93625],\n",
              " ['create', 0.93625],\n",
              " ['acknowledge', 0.93625],\n",
              " ['!passing judgement', 0.9318553611101094],\n",
              " ['give', 0.8984260493576671],\n",
              " ['tell you', 0.8824546694143958],\n",
              " ['people find useful', 0.8768503421599131],\n",
              " ['want', 0.8543221322870601],\n",
              " ['financial freedom', 0.8312891480214479],\n",
              " ['passing judgement', 0.8146183059958236],\n",
              " ['basic things solved', 0.7887338627786014],\n",
              " ['friends', 0.7787039511903949],\n",
              " ['raise', 0.7651093799969593],\n",
              " ['investmet game well', 0.7438314868380708],\n",
              " ['hit', 0.7105976198783038],\n",
              " ['nerve', 0.7105976198783038],\n",
              " ['something', 0.6816412722974622],\n",
              " ['equity ask', 0.679217134717464],\n",
              " ['understand', 0.6642731531688537],\n",
              " ['dreams', 0.6575553336716586],\n",
              " ['house', 0.6575553336716586],\n",
              " ['travel', 0.6317441765147713],\n",
              " ['notes', 0.6177792190673064],\n",
              " ['“', 0.6177792190673064],\n",
              " ['help first', 0.6058645241053096],\n",
              " ['saas business', 0.5702680987067659],\n",
              " ['eventually push forth', 0.5096004105439522],\n",
              " ['stop', 0.4897977123033273],\n",
              " ['great culture', 0.15000000000000002],\n",
              " ['accumulate capital', 0.15000000000000002],\n",
              " ['personal desire', 0.15000000000000002],\n",
              " ['private school proudly talk', 0.15000000000000002],\n",
              " ['way warren feels', 0.15000000000000002],\n",
              " ['tesla donate money', 0.15000000000000002],\n",
              " ['self actualization comes', 0.15000000000000002],\n",
              " ['fun working environment', 0.15000000000000002],\n",
              " ['alumni publications drive', 0.15000000000000002],\n",
              " ['new car', 0.15000000000000002],\n",
              " ['b2c ...', 0.15000000000000002],\n",
              " ['model citizen', 0.15000000000000002],\n",
              " ['least 1', 0.15000000000000002],\n",
              " ['helping others', 0.15000000000000002],\n",
              " ['growth machine', 0.15000000000000002],\n",
              " ['charitable agenda', 0.15000000000000002],\n",
              " ['causes make', 0.15000000000000002],\n",
              " ['humanly possible', 0.15000000000000002],\n",
              " ['think', 0.15000000000000002],\n",
              " ['’', 0.15000000000000002],\n",
              " ['year', 0.15000000000000002],\n",
              " ['investors', 0.15000000000000002],\n",
              " ['intelligent', 0.15000000000000002],\n",
              " ['ignore', 0.15000000000000002],\n",
              " ['humble', 0.15000000000000002],\n",
              " ['hood', 0.15000000000000002],\n",
              " ['heros', 0.15000000000000002],\n",
              " ['guys', 0.15000000000000002],\n",
              " ['going', 0.15000000000000002],\n",
              " ['generous', 0.15000000000000002],\n",
              " ['foster', 0.15000000000000002],\n",
              " ['fixing', 0.15000000000000002],\n",
              " ['feel', 0.15000000000000002],\n",
              " ['family', 0.15000000000000002],\n",
              " ['end', 0.15000000000000002],\n",
              " ['day', 0.15000000000000002],\n",
              " ['dances', 0.15000000000000002],\n",
              " ['dabble', 0.15000000000000002],\n",
              " ['customers', 0.15000000000000002],\n",
              " ['contribute', 0.15000000000000002],\n",
              " ['community', 0.15000000000000002],\n",
              " ['children', 0.15000000000000002],\n",
              " ['charity', 0.15000000000000002],\n",
              " ['cant', 0.15000000000000002],\n",
              " ['b2b', 0.15000000000000002],\n",
              " ['admit', 0.15000000000000002],\n",
              " ['15', 0.15000000000000002],\n",
              " ['10th', 0.15000000000000002],\n",
              " ['irrelevant', 0.15000000000000002],\n",
              " ['kind', 0.15000000000000002],\n",
              " ['life', 0.15000000000000002],\n",
              " ['scientist', 0.15000000000000002],\n",
              " ['willing', 0.15000000000000002],\n",
              " ['wife', 0.15000000000000002],\n",
              " ['wake', 0.15000000000000002],\n",
              " ['turn', 0.15000000000000002],\n",
              " ['trying', 0.15000000000000002],\n",
              " ['thanksgiving', 0.15000000000000002],\n",
              " ['successful', 0.15000000000000002],\n",
              " ['success', 0.15000000000000002],\n",
              " ['stress', 0.15000000000000002],\n",
              " ['spouse', 0.15000000000000002],\n",
              " ['sponsor', 0.15000000000000002],\n",
              " ['screaming', 0.15000000000000002],\n",
              " ['say', 0.15000000000000002],\n",
              " ['load', 0.15000000000000002],\n",
              " ['recognized', 0.15000000000000002],\n",
              " ['prospect', 0.15000000000000002],\n",
              " ['porsche', 0.15000000000000002],\n",
              " ['picnics', 0.15000000000000002],\n",
              " ['philanthropic', 0.15000000000000002],\n",
              " ['parents', 0.15000000000000002],\n",
              " ['nothing', 0.15000000000000002],\n",
              " ['need', 0.15000000000000002],\n",
              " ['much', 0.15000000000000002],\n",
              " ['moment', 0.15000000000000002],\n",
              " ['mentioned', 0.15000000000000002],\n",
              " ['meaningful', 0.15000000000000002],\n",
              " ['stick around 7 step process', 0.15000000000000002]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqDJzvs86BqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filter top keywords\n",
        "filtered_keywords = []\n",
        "for keyword in top_keywords:\n",
        "  if len(keyword[0].split()) > 1 and keyword[1] > 0.45:\n",
        "    # remove punctuation from keywords\n",
        "    filtered_keywords.append([keyword[0].translate(remove_punct_dict),keyword[1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJvpnaEv7wht",
        "colab_type": "code",
        "outputId": "8eef073e-630e-4363-d1f6-7701b12e3f5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        }
      },
      "source": [
        "# extracted keywords from dreams\n",
        "filtered_keywords"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['hit the nerve', 1.6970595408339264],\n",
              " ['travel the financial freedom', 1.6434831382341972],\n",
              " ['understand the investmet game', 1.6075978040249286],\n",
              " ['mention the house of their dreams\\n', 1.4444156452638794],\n",
              " ['tell these things', 1.389402560525686],\n",
              " ['raise your next round stop', 1.3718737942273989],\n",
              " ['raise your next round', 1.3533230859377086],\n",
              " ['give equity\\n', 1.3454783712681637],\n",
              " ['” notes', 1.3394415618653874],\n",
              " ['” “', 1.3394415618653874],\n",
              " ['people want', 1.3033697763297756],\n",
              " ['sell business', 1.301052613582438],\n",
              " ['solve order problems', 1.2666354758946907],\n",
              " ['eventually sell', 1.251883850658229],\n",
              " ['give Equity\\n', 1.2315863756768777],\n",
              " ['kids buy', 1.221296048809605],\n",
              " ['build something durable', 1.1888804377556572],\n",
              " ['solve first order problems', 1.1275],\n",
              " ['create jobs', 1.1275],\n",
              " ['acknowledge this person', 1.1275],\n",
              " ['retire my parenets', 1.1275],\n",
              " ['give away equity', 1.0988184018738951],\n",
              " ['close friends proud send kids', 1.0558744456325178],\n",
              " ['next round', 1.0198960275346058],\n",
              " ['10x revenue', 1.0],\n",
              " ['10x rev', 1.0],\n",
              " ['get thank', 1.0],\n",
              " ['everything else would', 1.0],\n",
              " ['fix one problem', 1.0],\n",
              " ['recognize I', 1.0],\n",
              " ['try I', 1.0],\n",
              " ['could fix one problem', 1.0],\n",
              " ['30x ebitda without bringing', 1.0],\n",
              " ['cool 5', 1.0],\n",
              " ['customer world', 1.0],\n",
              " ['interesting work', 1.0],\n",
              " ['get 1m', 1.0],\n",
              " ['build products', 0.9952023210659772],\n",
              " ['talented people', 0.9654577492232516],\n",
              " ['less churn', 0.9633889821661666],\n",
              " ['less cac', 0.9633889821661666],\n",
              " ['passing judgement', 0.9318553611101094],\n",
              " ['tell you', 0.8824546694143958],\n",
              " ['people find useful', 0.8768503421599131],\n",
              " ['financial freedom', 0.8312891480214479],\n",
              " ['passing judgement', 0.8146183059958236],\n",
              " ['basic things solved', 0.7887338627786014],\n",
              " ['investmet game well', 0.7438314868380708],\n",
              " ['equity ask', 0.679217134717464],\n",
              " ['help first', 0.6058645241053096],\n",
              " ['saas business', 0.5702680987067659],\n",
              " ['eventually push forth', 0.5096004105439522]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    }
  ]
}